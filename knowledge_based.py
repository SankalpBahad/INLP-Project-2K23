# -*- coding: utf-8 -*-
"""INLP Project - 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-hDahq-L4yfoJF6DAX3P3iXPBSHnIu3u

WIC TSV Dataset
"""

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('semcor')
nltk.download('wordnet')

from nltk.corpus import wordnet as wn
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.corpus import semcor
from nltk.corpus import wordnet
from nltk.tree import Tree

stop_words = set(stopwords.words('english'))

# sense_inventory={}
def preprocess_text(text):
    words = word_tokenize(text.lower())
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]
    return words

def lesk_algorithm(target_word, context):
    best_sense = None
    max_common_words = 0
    for sense in wn.synsets(target_word):
        gloss_words = preprocess_text(sense.definition())
        gloss_words.extend([w for ex in sense.examples() for w in preprocess_text(ex)])
        common_words = len(set(gloss_words) & set(context))
        if common_words > max_common_words:
            best_sense = sense
            max_common_words = common_words
    # if(best_sense not in sense_inventory[target_word]):
    # l=0
    # for i in sense_inventory[target_word]:
    #   if(i[0]==best_sense):
    #       i[1]+=1
    #       l=1
    #       break
    #   # sense_inventory[target_word]=[best_sense,0]
    #   # sense_inventory[target_word]
    # if(l==0):
    #   sense_inventory[target_word]=[best_sense,1]
    return best_sense

with open("test_examples.txt","r") as f:
  examples=f.read()
  examples=examples.split('\n')
  for i in range(len(examples)):
    examples[i]=examples[i].split('\t')
# print(examples)
with open("train_definitions.txt","r") as f:
  defs=f.read()
  defs=defs.split('\n')
# print(defs)

data={}
for i in range(len(examples)):
  data[examples[i][0]]=examples[i][2]
 
count_of_no_senses=0
count_of_same_definitions=0
index=0
for i in data.keys():
  text=data[i]
  target_word=i
  context = preprocess_text(text)
  predicted_sense = lesk_algorithm(target_word, context)
  # print("Target word: ", target_word)
  # print("Predicted sense: ", predicted_sense)

  if(predicted_sense):
    # print("Definition: ", predicted_sense.definition())
    if(predicted_sense.definition()==defs[index]):
      count_of_same_definitions+=1
  else:
    count_of_no_senses+=1
  index+=1

print("Percentage of entries having senses is",100 - (count_of_no_senses/len(data))*100)
# print("Percentage of entries having senses but same definitions is ",(count_of_same_definitions/count_of_no_senses)*100)