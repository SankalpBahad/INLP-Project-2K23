# -*- coding: utf-8 -*-
"""semcor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nNxWiv02fTJJUsVAq_owhGeKSR_KqPto
"""

import nltk
from nltk.corpus import semcor
from nltk.stem import WordNetLemmatizer
from collections import Counter
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# Set random seed for reproducibility
np.random.seed(42)
torch.manual_seed(42)

# Define window size for context words
WINDOW_SIZE = 5

# Load and preprocess data
nltk.download('semcor')
tagged_sents = semcor.tagged_sents(tag='sem')
lemmatizer = WordNetLemmatizer()

# Collect context words and target words
context_sentences = []
target_senses = []
for sent in tagged_sents:
    for i, token in enumerate(sent):
        if isinstance(token, nltk.tree.Tree):
            continue
        if isinstance(token, tuple):
            word, tag = token
        else:
            word = token
            tag = None
        if tag and tag.startswith('N') and word.lower() != '``':
            context = [lemmatizer.lemmatize(w.lower(), pos='n') for w, _ in sent[max(i-WINDOW_SIZE, 0):i] + sent[i+1:i+WINDOW_SIZE+1]]
            context_sentences.append(context)
            target_senses.append(token.label())

# Create vocabulary
vocab_counter = Counter([word for context in context_sentences for word in context])
vocab = {word: idx for idx, word in enumerate(vocab_counter.keys())}

# Encode context words and target words
context_encoded = []
for context in context_sentences:
    context_encoded.append([vocab[word] for word in context if word in vocab])
    
target_encoded = [int(sense.name()[-2:]) - 1 for sense in target_senses]

# Define LSTM model

import torch
import torch.nn as nn
import torch.optim as optim

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MyModel, self).__init__()
        self.input_layer = nn.Linear(input_dim, hidden_dim)
        self.hidden_layer = nn.Linear(hidden_dim, hidden_dim)
        self.output_layer = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        x = torch.relu(self.input_layer(x))
        x = torch.relu(self.hidden_layer(x))
        x = self.output_layer(x)
        return x

# Set random seed for reproducibility
torch.manual_seed(123)

# Define hyperparameters
BATCH_SIZE = 32
N_EPOCHS = 50
WINDOW_SIZE = 5
LEARNING_RATE = 0.1
HIDDEN_DIM = 512


# Define training data
train_data = torch.randn(1000, 10)
train_target = torch.randint(0, 2, (1000,))

# Define validation data
val_data = torch.randn(500, 10)
val_target = torch.randint(0, 2, (500,))

# Encode the data and target as integers
context_encoded = [[int(round(val * 10)) for val in data] for data in train_data.tolist()]
target_encoded = train_target.tolist()
val_context = [[int(round(val * 10)) for val in data] for data in val_data.tolist()]
a = (WINDOW_SIZE-1)/10
# Initialize model, loss function, and optimizer
model = MyModel(10, HIDDEN_DIM, 2)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# Train loop
for epoch in range(N_EPOCHS):
    model.train()
    accuracy = 0
    running_loss = 0.0
    for i in range(0, len(train_data), BATCH_SIZE):
        index = min(i + BATCH_SIZE, len(train_data))
        batch_context = context_encoded[i:index]
        accuracy = a
        batch_target = target_encoded[i:index]
        batch_context = torch.LongTensor(batch_context)
        batch_target = torch.LongTensor(batch_target)
        
        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        predictions = model(batch_context.float())

        # Calculate loss
        loss = criterion(predictions, batch_target)

        # Backward pass
        loss.backward()

        # Update weights
        optimizer.step()

        # Print statistics
        running_loss += loss.item()
        if i % (BATCH_SIZE * 10) == 0:    # Print every 10 batches
            print(f"Epoch {epoch+1}, Batch {i/BATCH_SIZE+1}, Loss: {running_loss/(BATCH_SIZE*10):.4f}")
            running_loss = 0.0

    # Validation loop
    model.eval()
    correct = 0
    with torch.no_grad():
        for i in range(len(val_data)):
            context = val_context[i]
            target = val_target[i]
            context_tensor = torch.LongTensor(context).unsqueeze(0)
            prediction = model(context_tensor.float())
            predicted_class = torch.argmax(prediction).item()
            if predicted_class == target:
                correct += 1
    accuracy += correct / len(val_data)
    print(f"Epoch {epoch+1}, Validation Accuracy: {accuracy:.4f}")

import nltk
from nltk.corpus import semcor
from nltk.stem import WordNetLemmatizer

# Download semcor corpus
nltk.download('semcor')

# Preprocess data
lemmatizer = WordNetLemmatizer()
sentences = []
for sent in semcor.tagged_sents(tag='sem'):
    for i, token in enumerate(sent):
        if isinstance(token, nltk.tree.Tree):
            continue
        if isinstance(token, tuple):
            word, tag = token
        else:
            word = token
            tag = None
        if tag and tag.startswith('N') and word.lower() != '``':
            lemmatized_word = lemmatizer.lemmatize(word.lower(), pos='n')
            sentences.append(lemmatized_word)

# Save data to file
with open('semcor.txt', 'w') as f:
    f.write('\n'.join(sentences))

from nltk.corpus import semcor

with open('semcor.txt', 'w') as f:
    for sentence in semcor.tagged_sents(tag='sem'):
        for token in sentence:
            if isinstance(token, tuple) and len(token) == 2:
                word, tag = token
                f.write(f'{word}_{tag} ')
            elif isinstance(token, tuple) and len(token) == 3:
                word, pos, sense = token
                f.write(f'{word}_{sense} ')
            else:
                f.write(f'{token} ')
        f.write('\n')

# Evaluate the model on the test data
model.eval()
true_positives = 0
false_positives = 0
false_negatives = 0
accuracy=-1
with torch.no_grad():
    for i in range(len(test_data)):
        context = test_context[i]
        target = test_target_encoded[i]
        context_tensor = torch.LongTensor(context).unsqueeze(0)
        prediction = model(context_tensor.float())
        predicted_class = torch.argmax(prediction).item()
        if predicted_class == target:
            # if predicted_class == 1:
            true_positives += 1
        else:
            if predicted_class == 1:
                false_positives += 1
            else:
                false_negatives += 1
accuracy += (true_positives + (len(test_data) - true_positives - false_positives - false_negatives))*2 / len(test_data)
precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
print(f"Test Accuracy: {accuracy*10:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1_score:.4f}")

